{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##  Abalone Age Prediction\nDescription- Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem. ","metadata":{"_uuid":"7019c9eb42385dabf10c857f610e93d5916e7b88"}},{"cell_type":"markdown","source":"In this analysis I have focused on exploratory data analysis on Abalone Dataset. \n![image info](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2F12pTskC7ORQB9e%2Fgiphy.gif&f=1&nofb=1)","metadata":{"_uuid":"c541635be5add63a3737cb7cc4f251a442b6f584"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n%matplotlib inline\n\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.model_selection import  train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.linear_model import  Ridge\nfrom sklearn.svm import SVR\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"./datasets/\"))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the dataset (you can also read it from here: https://raw.githubusercontent.com/fenago/MLEssentials2/main/datasets/abalone.csv )\ndata = pd.read_csv('./datasets/abalone.csv')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n- Can we create, combine, drop and features from the dataset?\n- From problem statement and feature discription, let's first compute the target variable of the problem ' Age' and assign it to the dataset. \nAge = 1.5+Rings\n- This is an example of feature engineering.","metadata":{"_uuid":"d7b6e89c41a7647df93b25706d0baabc31143fb9"}},{"cell_type":"code","source":" data['age'] = data['Rings']+1.5\n data.drop('Rings', axis = 1, inplace = True)","metadata":{"_uuid":"aae219dda94383aa845d7a6e9d80c35b9d94a74f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univariate analysis\nUnderstanding feature wise statistics using various inbuilt tools ","metadata":{"_uuid":"3fd54e6744951848fbe64fe63dbc79b2856fdd97"}},{"cell_type":"code","source":"print('This dataset has {} observations with {} features.'.format(data.shape[0], data.shape[1]))","metadata":{"_uuid":"2673188ba421c44e2209d6e3769d3750255eed82","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"_uuid":"7abee39862538dbf9d20b0ee5d331a3fc9a7c3ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"_uuid":"3af7c59489d337f024a2b5c065e71181b62548e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"_uuid":"93d42e2c87393a6db6b8fa18b1fc2f3b0967957a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key insights : \n            - No missing values in the dataset\n            - All numerical features but 'sex'\n            - Though features are not normaly distributed, are close to normality\n            - None of the features have minimum = 0 except Height (requires re-check)\n            - Each feature has difference scale range","metadata":{"_uuid":"68e3999cb400e4f12374f283f0e2581b5cb9077e"}},{"cell_type":"code","source":"data.hist(figsize=(20,10), grid=False, layout=(2, 4), bins = 30)","metadata":{"_uuid":"cbad8ace48d984df41c92ec10b789d9e51a957ce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = data.select_dtypes(include=[np.number]).columns\ncategorical_features = data.select_dtypes(include=[np.object]).columns","metadata":{"_uuid":"a6abec3784ae308f4a6389f10a2e3ee012182cf3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features","metadata":{"_uuid":"c3b436f508202f02fe0005ac8f858da5a769c15c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features","metadata":{"_uuid":"cb034d4b75c8d8e023f8bee544422f3b5e03246d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_values = skew(data[numerical_features], nan_policy = 'omit')\ndummy = pd.concat([pd.DataFrame(list(numerical_features), columns=['Features']), \n           pd.DataFrame(list(skew_values), columns=['Skewness degree'])], axis = 1)\ndummy.sort_values(by = 'Skewness degree' , ascending = False)","metadata":{"_uuid":"dae3a06f78828b6742289219e8152e8d8072dad4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For normally distributed data, the skewness should be about 0. For unimodal continuous distributions, a skewness value > 0 means that there is more weight in the right tail of the distribution. The function skewtest can be used to determine if the skewness value is close enough to 0, statistically speaking.\n        - Height has highest skewedness followed by age, Shucked weight (can be cross verified through histogram plot)","metadata":{"_uuid":"4a3c1617c524c624841f02defa4c5541ec3ee5b5"}},{"cell_type":"code","source":"# Missing values\nmissing_values = data.isnull().sum().sort_values(ascending = False)\npercentage_missing_values = (missing_values/len(data))*100\npd.concat([missing_values, percentage_missing_values], axis = 1, keys= ['Missing values', '% Missing'])","metadata":{"_uuid":"01c3f04cba9ee61dced6bce0fc56a8b5c729bd83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values as said before","metadata":{"_uuid":"f3620f82f44db82277bbe1fcaef249785a79372f"}},{"cell_type":"code","source":"sns.countplot(x = 'Sex', data = data, palette=\"Set3\")","metadata":{"_uuid":"d7c28a8e7ed7e00c64200e5b02ad963a601b3c42","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,7))\nsns.swarmplot(x = 'Sex', y = 'age', data = data, hue = 'Sex')\nsns.violinplot(x = 'Sex', y = 'age', data = data)","metadata":{"_uuid":"3277f487c80090a513785ea9f1f8471add392006","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"        Male : age majority lies in between 7.5 years to 19 years\n        Female: age majority lies in between 8 years to 19 years\n        Immature: age majority lies in between 6 years to < 10 years","metadata":{"_uuid":"0a6bedf3c396d9b6daa8380ccb7f6d7c7f1c3b6a"}},{"cell_type":"code","source":"data.groupby('Sex')[['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n       'Viscera weight', 'Shell weight', 'age']].mean().sort_values('age')","metadata":{"_uuid":"7e70607040f0bd467b19d4cf7f385af94f2f2279","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bivariate Analysis\nBivariate analysis is vital part of data analysis process for, it gives clear picture on how each features are affected in presence of other features.  \nIt also helps us understand and identify significance features, overcome multi-collinearity effect, inter-dependency and thus, provides insights on hidden data noise pattern.","metadata":{"_uuid":"c700e66b729aad1e88749c41bbde87fa2f4fa9b6"}},{"cell_type":"code","source":"sns.pairplot(data[numerical_features])","metadata":{"_uuid":"c4e4159c1ca6d07636089b433f1583a506b72d77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### key insights\n            - length is linearly correlated with diameter \n            - while, non-linear relation with height, whole weight, shucked weight, viscera weight and shell weight  ","metadata":{"_uuid":"5c96d7ded4500d2701d9db10e3f0cadae4b1f797"}},{"cell_type":"code","source":"plt.figure(figsize=(20,7))\nsns.heatmap(data[numerical_features].corr(), annot=True)","metadata":{"_uuid":"c48f9a8832bef25271deed243c0f850bb0985483","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"        Whole Weight is almost linearly varying with all other features except age\n        Heigh has least linearity with remaining features\n        Age is most linearly proprtional with Shell Weight followed by Diameter and length\n        Age is least correlated with Shucked Weight\n        \n  Such high correlation coefficients among features can result into multi-collinearity. We need to check for that too, however, I have not done it here.","metadata":{"_uuid":"3c5f9a90f78fbafa5070b14a52996285d628596b"}},{"cell_type":"markdown","source":"## Outliers handlings","metadata":{"_uuid":"f35c6e25824de2fa6447d023dc5a349319d22746"}},{"cell_type":"code","source":"data = pd.get_dummies(data)\ndummy_data = data.copy()","metadata":{"_uuid":"8cf41a980fb08dfe6f15e07f1bab5b73a35ac3e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.boxplot( rot = 90, figsize=(20,5))","metadata":{"_uuid":"32658fecce28bc0a71e78687fea1589280e2d638","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Viscera weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"e966754fdf26e2c168c6a0d3d4809bb923ef3302","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outliers removal\ndata.drop(data[(data['Viscera weight']> 0.5) & (data['age'] < 20)].index, inplace=True)\ndata.drop(data[(data['Viscera weight']<0.5) & (data['age'] > 25)].index, inplace=True)","metadata":{"_uuid":"820b5e081b8eba31dda76255118d2f480be2980c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Shell weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"d202787ddba0ea04703b32cfb3c11d0b8fb125cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Shell weight']> 0.6) & (data['age'] < 25)].index, inplace=True)\ndata.drop(data[(data['Shell weight']<0.8) & (data['age'] > 25)].index, inplace=True)","metadata":{"_uuid":"6b5c121e6ed26feaa13a02eb7e342390d5ce716f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Shucked weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"9e163356124f8899d23a6095311b062b51994f73","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Shucked weight']>= 1) & (data['age'] < 20)].index, inplace=True)\ndata.drop(data[(data['Shucked weight']<1) & (data['age'] > 20)].index, inplace=True)","metadata":{"_uuid":"05d95138b39174a1c7b225c70576111c3eb4235a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Whole weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"2d8037f5406af07090b148d6de0507d393d6995b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Whole weight']>= 2.5) & (data['age'] < 25)].index, inplace=True)\ndata.drop(data[(data['Whole weight']<2.5) & (data['age'] > 25)].index, inplace=True)","metadata":{"_uuid":"71ba33c6841ab15e5397c4d6b7b8f819be09c649","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Diameter'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"2c9571d7bf19a43b87953a682e072142a20af62e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Diameter']<0.1) & (data['age'] < 5)].index, inplace=True)\ndata.drop(data[(data['Diameter']<0.6) & (data['age'] > 25)].index, inplace=True)\ndata.drop(data[(data['Diameter']>=0.6) & (data['age']< 25)].index, inplace=True)","metadata":{"_uuid":"626eaa47a20bd430f5d4e721198504ca0456504b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Height'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"f7ff8412dc3d2be1228e214800e7eb6df50823e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Height']>0.4) & (data['age'] < 15)].index, inplace=True)\ndata.drop(data[(data['Height']<0.4) & (data['age'] > 25)].index, inplace=True)","metadata":{"_uuid":"34d3964676d1bf17a08d4f5e15f71741931368ef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'Length'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","metadata":{"_uuid":"52172558fbcc8dc1947823a4519b5394272d65e2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(data[(data['Length']<0.1) & (data['age'] < 5)].index, inplace=True)\ndata.drop(data[(data['Length']<0.8) & (data['age'] > 25)].index, inplace=True)\ndata.drop(data[(data['Length']>=0.8) & (data['age']< 25)].index, inplace=True)","metadata":{"_uuid":"1545bc72f83d245023c0abd1a94a6b9bac5ec8cc","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing, Modeling, Evaluation\nThe base steps followed in any data modeling pipelines are:\n               - pre-processing \n               - suitable model selection\n               - modeling\n               - hyperparamaters tunning using GridSearchCV\n               - evaluation","metadata":{"_uuid":"3e86b69d2eae03347aacbdc855825acadd43c792"}},{"cell_type":"code","source":"X = data.drop('age', axis = 1)\ny = data['age']","metadata":{"_uuid":"8748d1c32064154468e3df477ebe64f2383f1ad3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardScale = StandardScaler()\nstandardScale.fit_transform(X)\n\nselectkBest = SelectKBest()\nX_new = selectkBest.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.25)","metadata":{"_uuid":"f76f0ad0c0f12b88496d79fbd6a173a4aa5e0fb0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(10)\ndef rmse_cv(model, X_train, y):\n    rmse =- (cross_val_score(model, X_train, y, scoring='neg_mean_squared_error', cv=5))\n    return(rmse*100)\n\nmodels = [LinearRegression(),\n             Ridge(),\n             SVR(),\n             RandomForestRegressor(),\n             GradientBoostingRegressor(),\n             KNeighborsRegressor(n_neighbors = 4),]\n\nnames = ['LR','Ridge','svm','GNB','RF','GB','KNN']\n\nfor model,name in zip(models,names):\n    score = rmse_cv(model,X_train,y_train)\n    print(\"{}    : {:.6f}, {:4f}\".format(name,score.mean(),score.std()))","metadata":{"_uuid":"2719b5f2ca2a1398e7e4eac8cd939efda8ecfe4f","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You have seen the performance of each one of above models.\n\nSo, according to you which model should we start or choose?\nWell the answer lies in Occam's razor principle from philosophy https://simple.wikipedia.org/wiki/Occam%27s_razor.\" Suppose there exist two explanations for an occurrence. In this case the simpler one is usually better. Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation.\"\nHence, starting with the simplest model Ridge, for various reasons:\n            - Feature Dimension is less\n            - No misisng values\n            - Few categorical features","metadata":{"_uuid":"21d1e8553cb3c2c23013fd4111dc139dc9878111"}},{"cell_type":"code","source":"\ndef modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['age'])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = -cross_val_score(alg, dtrain[predictors], dtrain['age'], cv=cv_folds, \n                                                    scoring='r2')\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print( \"RMSE : %.4g\" % mean_squared_error(dtrain['age'].values, dtrain_predictions))\n    print( \"R2 Score (Train): %f\" % r2_score(dtrain['age'], dtrain_predictions))\n    \n    if performCV:\n        print( \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),\n                                                                                 np.min(cv_score),np.max(cv_score)))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.coef_, predictors).sort_values(ascending=False)\n        plt.figure(figsize=(20,4))\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')","metadata":{"_uuid":"adf0d1897b464aca95e96109999da3022ad86fdd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Base Model\npredictors = [x for x in data.columns if x not in ['age']]\nlrm0 = Ridge(random_state=10)\nmodelfit(lrm0, data, predictors)","metadata":{"_uuid":"5458fb52e377940e95207bd731c76acfc677521f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tunning using GrideSearchCV","metadata":{"_uuid":"f763b30aebcc52e636d6d21f36795cf3b0e96084"}},{"cell_type":"code","source":"# Let's do hyperparameter tunning using GrideSearchCV\nfrom sklearn.model_selection import  GridSearchCV\nparam  = {'alpha':[0.01, 0.1, 1,10,100],\n         'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\nglrm0 = GridSearchCV(estimator = Ridge(random_state=10,),\nparam_grid = param,scoring= 'r2' ,cv = 5,  n_jobs = -1)\nglrm0.fit(X_train, y_train)\nglrm0.best_params_, glrm0.best_score_","metadata":{"_uuid":"6014efdc79f204d14b1860e90155732bbfc1870e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelfit(Ridge(alpha = 0.1,random_state=10,), data, predictors)","metadata":{"_uuid":"ed5e3a93f4d4a4c6c22b155a4c3a39679ba5f497","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CV score has improved slightly while, R2_score has decreased showing base model was overfitted.\nUsing above process multiple options can be tried to could up with much more robust model.\nThis process can also be tried on different models : RF, GB, etc.","metadata":{"_uuid":"f97351fa2eb3d13877efa4f8d441696750ee529e"}},{"cell_type":"markdown","source":"Hyperparameter tunning is an iterative process and it can go on. As this kernal primary focuses on EDA of Abalone dataset, modeling building will be taken into account later in the course. Hope I have helped you getting insights of Abalone dataset through this notebook.","metadata":{"_uuid":"91c437847b4727134288f1dd969d102eb413b481"}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"191d171700abf3256b4132108c125d482cdd1126"}}]}