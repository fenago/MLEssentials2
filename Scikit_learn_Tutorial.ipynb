{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Scikit-Learn with Python","metadata":{}},{"cell_type":"markdown","source":"### In this tutorial, we will learn about different models of Scikit-learn library. It is a library of python for developing Machine Learning Models and Statistical Models.\n\nAfter careful reading and practicing this tutorial book you will be able to implement models like Regression, Classification and Clustering etc according to your problem statement.\n\n\n#### This library written in Python and is built upon NumPy, Scipy and Matplotlib.\n","metadata":{}},{"cell_type":"markdown","source":"## Pre-requisite of Scikit-Learn\n\n#### Before jumping into the discussion of models in this library, one should have a basic knowledge of Python Language, Machine learning, Numpy, Scipy and Matplotib. Obviously, having a sound knowledge of above mentioned concepts will be a positive point for him/her. But if you are new to these then it is highly recommended to have a basic information of above mentioned libraries and concepts.","metadata":{}},{"cell_type":"markdown","source":"### Assuming that you have installed Python in your system as well as Scikit-learn and related libraries.","metadata":{}},{"cell_type":"markdown","source":"### Now we will start working on how to import scikit-learn library and others and their way of usage.","metadata":{}},{"cell_type":"markdown","source":"Let's import the Scikit-Learn Library","metadata":{}},{"cell_type":"code","source":"import sklearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By executing above given command, one can import this library. Importing any library means, we can use all the built-in functions of that library in our script. \n\n## Datasets\nBest thing about SKLEARN is, it provides few datasets which can be used for practice and better understanding of Machine Learning Concepts.\n\nSo, Let's import those datasets too by the following command.","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This library, provides different datasets like iris_dataset, diabetes, digits, Boston House-price Dataset and linnerud dataset for different regression or classification purpose.\n\nAt very first step we will start working on concept of classification. \n\n# Classification\n\nIn machine learning, classification refers to a predictive modeling problem where a class label is predicted for a given example of input data.\nIn simple words, when we have any input example and we need to predict whether it belongs to Class A or Class B, then in this case we use classification methods.\n\nLet's learn more about Classification with examples","metadata":{}},{"cell_type":"markdown","source":"First, let's load iris dataset","metadata":{}},{"cell_type":"code","source":"myIrisData = datasets.load_iris()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### About Iris Dataset\nIt is necessary to know about the dataset before using it in our code.\n\nIris dataset is related to Iris Plant Dataset. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. It has 4 Numeric, predictive Attributes and the class. \nAttributes are as follow: \n\n- sepal length in cm\n- sepal width in cm\n- petal length in cm\n- petal width in cm\n\nand the classes to which these instances belongs are:\n\n- Iris-Setosa\n- Iris-Versicolour\n- Iris-Virginica","metadata":{}},{"cell_type":"code","source":"myIrisData.feature_names","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myIrisData.target_names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myIrisData.target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myIrisData.data[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above cells, you can see features names, target names, target classes and data of feature's instances respectively.\n\nThe above data is not easy to understand. We don't know which column represent what data. Let's Import panda library and create a dataframe of the imported iris dataset to better understand the data.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(myIrisData.data, columns=myIrisData.feature_names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will create two vectors. One of the data/ features and the other of the target/ labels.","metadata":{}},{"cell_type":"code","source":"X = myIrisData.data\nY = myIrisData.target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting and Shuffling Dataset\n\nNow, It's time to split the data randomly for training and testing purpose. For this reason, we have a built-in function named train_test_split() which is provided by sklearn library. This helps us to shuffle the data randomly and split it into 2 parts, i.e train and test part. \n\nShuffling the data is necessary to avoid biasness or in other words to ensure that each data point creates an \"independent\" change on the model, without being biased by the same points before them. Also data set is splitted into 70:30 ratio (70% train set and 30% test set). You can change the ratio according to your need.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In above cell, we are importing the function named train_test_split first from sklearn.model_selection. This function help us to split the data in train and test chunks randomly with the ratio of 70:30. Test size will determine the ratio of testing and training chunks. if you want to split the data in 80:20 ratio, then you have to give test_size = 0.2. Random_state = 1 determine that function will split data randomly.\n\nNow, let's check the shape of the vectors.","metadata":{}},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\n\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression is used for classification. For example, given a data instance, predict its flower class.\n\nWith the help of sklearn we can easily implement Logistic Regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_predicted = logreg.predict(X_test)\ny_predicted","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"After Classification, model needs to be evaluated how good model is predicting. \n\nScikit learn provides <b>metrics</b> module for calculating different metrices for a model. Confusion Matrix can be imported from <b>metrics</b> for computing accuracy of our model.","metadata":{}},{"cell_type":"markdown","source":"Using the results from above example of Logistic Regression for calculating the accuracy of model and other metrices.","metadata":{}},{"cell_type":"markdown","source":"To compute the accuracy, scikit learn provides <b>accuracy_score</b> function.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predicted)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Confusion Matrix</b> is used for evaluation of a model with respect to class labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_predicted)\nprint(confusion_matrix)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, above matrix is called Confusion Matrix. But it is not easily interpretable.\n\nWe can display this confusion matrix using ConfusionMatrixDisplay.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay(confusion_matrix).plot()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sometimes, accuracy is not a good metric for model evaluation, we need other measures like f1-score, precision and recall as well.\n\nScikit learn provides a very useful function for computing all these metrices in a single line.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predicted))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN (k nearest neighbors)","metadata":{}},{"cell_type":"markdown","source":"Let's explore another classification model that is <b>KNN Classifier</b>.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nclassifier_knn = KNeighborsClassifier(n_neighbors = 3)\nclassifier_knn.fit(X_train, y_train)\ny_pred = classifier_knn.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is how you can train the model on KNeighborsClassifier and predict the model on splitted test data. \n\nFinding accuracy by comparing actual response values(y_test)with predicted response value(y_pred)","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see the test model predicted with the accuracy of 97.7%\n\nProviding sample data and the model will make prediction out of that data\n","metadata":{}},{"cell_type":"code","source":"sample = [[4, 4, 3, 5], [2, 4, 3, 5]]\npreds = classifier_knn.predict(sample)\n\nfor p in preds:\n    pred_species = myIrisData.target_names[p]\n    print(\"Predictions:\", pred_species)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's Change the sample data and test again.","metadata":{}},{"cell_type":"code","source":"sample = [[5, 4, 3, 5], [3, 4, 2, 4]]\npreds = classifier_knn.predict(sample)\n\nfor p in preds:\n    pred_species = myIrisData.target_names[p]\n    print(\"Predictions:\", pred_species)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obviously in practical life, no one train the model again and again for each testing. This train model need to be stored for prediction.\nThere is a way you can save the model and load it again for further practice.\n\nLet's see how can  we do this.\n","metadata":{}},{"cell_type":"code","source":"import joblib\njoblib.dump(classifier_knn, 'iris_classifier_knn.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model will saved as a file with extension .joblib and can be loaded again.","metadata":{}},{"cell_type":"code","source":"joblib.load('iris_classifier_knn.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now next step is to learn about Linear Regression.","metadata":{}},{"cell_type":"markdown","source":"# Linear Regression\n\nThis is a supervised ML model. It is one of the best statistical models that studies the relationship between a dependent variable (Y) with a given set of independent variables (X). In other words, this is used when the output variable is continuous and it follows linear relation with dependent variables. It can be used to forecast sales in the coming months by analyzing the sales data for previous months.\n\nSklearn helps to implement Linear Regression model easily","metadata":{}},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"markdown","source":"For linear regression, lets load Boston Housing dataset. There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features.\n\nScikit learn provides sample of this dataset in its datasets module.","metadata":{}},{"cell_type":"code","source":"boston = datasets.load_boston()\nprint(boston.DESCR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(boston.data.shape)\nprint(boston.feature_names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's better understand our data in pandas.","metadata":{}},{"cell_type":"code","source":"boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)\nboston_df['PRICE'] = boston.target\nboston_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will make independent(X) and dependent (Y) variable of our data.","metadata":{}},{"cell_type":"code","source":"X = boston.data\ny = boston.target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split our dataset in train and test parts.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nregression_model = LinearRegression()\n\nregression_model.fit(X_train, Y_train)\n\ny_predicted = regression_model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"Model evaluation for regression is different as compared to model evaluation for classification.\n\nIn case of regression, we compute error score of predicted values from the actual ones. There are different error scores which we can compute to evaluate our model. Lets explore a few of them.","metadata":{}},{"cell_type":"markdown","source":"### Mean Squared Error:","metadata":{}},{"cell_type":"markdown","source":"The MSE is a measure of the quality of an estimator. As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse = mean_squared_error(Y_test, y_predicted)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### r2_score:","metadata":{}},{"cell_type":"markdown","source":"Best possible score is 1.0 and it can be negative in case of worse model. A constant model that always predicts the expected value of y, disregarding the input features, would get a score of 0.0.\n\nIt represents the proportion of variance of y (independent variables) in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.","metadata":{}},{"cell_type":"markdown","source":"#### Syntax:\n\nsklearn.metrics.r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2 = r2_score(Y_test, y_predicted)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model performance: \")\nprint(f'RMSE is {rmse}')\nprint(f'R2 score is {r2}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# Clustering","metadata":{}},{"cell_type":"markdown","source":"Cluster analysis is an <b>unsupervised</b> technique used in machine learning that attempts to find clusters of observations within a dataset.\n\nThe goal of cluster analysis is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.","metadata":{}},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"markdown","source":"For clutering we will generate a dummy data. Scikit learn provides <b>make_blobs</b> function in its <b>datasets</b> module to generate dummy data for clustering and classification.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=150, centers=3, n_features=3,\n                  random_state=0)\nprint(X.shape)\nprint(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have generated 150 data points with 3 features. This dataset contains 3 clusters (centers) and we know this as a fact because we have generated this dataset. But in real, we do not know how many clusters are there in our data.","metadata":{}},{"cell_type":"markdown","source":"We will use K-means clutering algorithm for clustering our data. Scikit learn provides support for different clustering alogorithms. Wel will import our kmeans clustering algorithm from sklearn.cluster","metadata":{}},{"cell_type":"code","source":"from sklearn import cluster\n\nkmeans = cluster.KMeans(n_clusters=2) #asking the algorithm to make 2 clusters in our data\nkmeans.fit(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.labels_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In above code, we asked our algorithm to create 2 clusters in our data. Algorithm doesn't know how many clusters are there in data, but it will create clusters according to our given number. \n\nHere 0,1 doesn't mean the class label, 0 and 1 correspondts to respective cluster. It doesn't have anything to do with class label.","metadata":{}},{"cell_type":"markdown","source":"Now let's try with 3 clusters and more.","metadata":{}},{"cell_type":"code","source":"kmeans = cluster.KMeans(n_clusters=3) #asking the algorithm to make 3 clusters in our data\nkmeans.fit(X)\nkmeans.labels_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = cluster.KMeans(n_clusters=5) #asking the algorithm to make 5 clusters in our data\nkmeans.fit(X)\nkmeans.labels_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way we can do clustering analysis of our data. Clustering algorithms are hard to evaluate","metadata":{}},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"raw","source":"In this tutorial we have learned the following concepts:\n1- Classification\n2- Regression\n3- Clustering\n4- Datasets in scikit learn\n5- Splitting of dataset into train and test sets\n6- Model training in scikit learn\n7- Model evaluations for classification and regression\n8- Making dataframes in pandas","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}